#!/bin/bash
### partition 1: RM [default,use one or more full nodes], 
### partition 2: RM-shared [use only part of one node] 
### partition 3: RM-512 [use one or more full 512GB nodes]
#SBATCH --partition=RM-shared # 
#SBATCH --nodes=1
#SBATCH --ntasks=100
#SBATCH --time=1:00:00
#SBATCH --mail-user=duc.nguyen@uconn.edu      # Destination email address
#SBATCH --mail-type=ALL                       # Event(s) that triggers email notification
#SBATCH --job-name=dedalus_job
#SBATCH --output=dedalus_output_%j
export SLURM_EXPORT_ENV=ALL
#export I_MPI_FABRICS=shm,tcp

##the slurm number to restart simulation... This need full state to be stored.
SUBMITDIR=$SLURM_SUBMIT_DIR
WORKDIR=/ocean/projects/phy240052p/vnguyen9/testing/matlab_$SLURM_JOB_ID
mkdir -p "$WORKDIR" && cp -r *.py "$WORKDIR" && cp submit_dedalus_bridges2 "$WORKDIR" && cd "$WORKDIR" || exit -1

source activate base
conda activate dedalus3
mpiexec -n $SLURM_NTASKS python3 RBC.py

cd "$SUBMITDIR" && cp dedalus_output_$SLURM_JOB_ID "$WORKDIR"

